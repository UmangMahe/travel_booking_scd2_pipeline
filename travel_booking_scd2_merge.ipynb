{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c0beb3-f917-4a44-879e-c5c34bfdd160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63001c12-46ba-4402-8115-849c22fbbb71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(os.environ.get('SPARK_VERSION'))\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "\n",
    "booking_data = f\"/Volumes/incremental_load/default/orders_data/booking_data/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/incremental_load/default/orders_data/customer_data/customers_{date_str}.csv\"\n",
    "\n",
    "print(booking_data)\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591f98e4-e7af-4ea2-8e59-bbc9976b574d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(customer_data)\n",
    "customer_df.printSchema()\n",
    "display(customer_df)\n",
    "\n",
    "booking_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(booking_data)\n",
    "\n",
    "booking_df.printSchema()\n",
    "display(booking_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2d9665-c339-4f1e-b09b-b8dc1f257435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality checks for customer_df\n",
    "customer_check = Check(spark, CheckLevel.Error, \"Customer Data Quality Check\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isUnique(\"customer_id\") \\\n",
    "    .isComplete(\"customer_name\") \\\n",
    "    .isComplete(\"customer_address\") \\\n",
    "    .isComplete(\"email\") \\\n",
    "    .hasSize(lambda x: x > 0)\n",
    "\n",
    "customer_verification = VerificationSuite(spark) \\\n",
    "    .onData(customer_df) \\\n",
    "    .addCheck(customer_check) \\\n",
    "    .run()\n",
    "\n",
    "customer_df_check = VerificationResult.checkResultsAsDataFrame(spark, customer_verification)\n",
    "print(customer_df_check)\n",
    "\n",
    "# Data quality checks for booking_df\n",
    "booking_check = Check(spark, CheckLevel.Error, \"Booking Data Quality Check\") \\\n",
    "    .isComplete(\"booking_id\") \\\n",
    "    .isUnique(\"booking_id\", hint=\"Booking ID is not unique throught\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isNonNegative(\"amount\") \\\n",
    "    .isNonNegative(\"quantity\") \\\n",
    "    .isNonNegative(\"discount\") \\\n",
    "    .hasSize(lambda x: x > 0)\n",
    "\n",
    "booking_verification = VerificationSuite(spark) \\\n",
    "    .onData(booking_df) \\\n",
    "    .addCheck(booking_check) \\\n",
    "    .run()\n",
    "\n",
    "booking_df_check = VerificationResult.checkResultsAsDataFrame(spark, booking_verification)\n",
    "print(booking_df_check)\n",
    "\n",
    "if booking_verification.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Booking Data\")\n",
    "\n",
    "if customer_verification.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Customer Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a2e750-d4ad-4330-8569-b0b4afec814b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "# Join booking data with customer data on customer_id\n",
    "df_joined = booking_df_incremental.join(customer_df, \"customer_id\")\n",
    "# Transform and get Net amount after discount\n",
    "df_transformed = df_joined.withColumn(\n",
    "    \"net_amount\", col(\"amount\") - col(\"discount\")) \\\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "\n",
    "# Group by aggregation by customer id and booking type\n",
    "df_transformed_agg = df_transformed \\\n",
    "    .groupBy(\"customer_id\", \"booking_type\") \\\n",
    "    .agg(\n",
    "        _sum(\"net_amount\").alias(\"total_amount\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09423c1a-cdf8-4320-b15f-b6bbf80a71e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_table_path = \"incremental_load.default.booking_fact\"\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path)\n",
    "\n",
    "if fact_table_exists:\n",
    "    existing_fact_df = spark.read.format(\"delta\").table(fact_table_path)\n",
    "    df_combined = existing_fact_df.unionByName(df_transformed_agg, allowMissingColumns = True)\n",
    "\n",
    "    df_final_agg = df_combined.groupBy(\"customer_id\", \"booking_type\") \\\n",
    "        .agg(\n",
    "            _sum(\"total_amount\").alias(\"total_amount\"),\n",
    "            _sum(\"total_quantity\").alias(\"total_quantity\")\n",
    "        )\n",
    "else:\n",
    "    df_final_agg = df_transformed_agg\n",
    "\n",
    "print(df_final_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c536ff25-b65b-4b91-8f85-6a48b4bc6154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_agg.write \\\n",
    ".format(\"delta\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"overwriteSchema\", \"true\") \\\n",
    ".saveAsTable(fact_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984670c9-dd4b-47ec-9aba-91d4ae797ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd2_table_path = \"incremental_load.default.customer_dim\"\n",
    "scd2_table_exists = spark._jsparkSession.catalog().tableExists(scd2_table_path)\n",
    "\n",
    "if not scd2_table_exists:\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(scd2_table_path)\n",
    "else:\n",
    "    deltaTable = DeltaTable.forName(spark, scd2_table_path)\n",
    "    print(deltaTable.toDF())\n",
    "   \n",
    "    # SCD2 Merge\n",
    "    deltaTable.alias(\"scd\").merge(\n",
    "        customer_df.alias(\"updates\"),\n",
    "        \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\" \n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"valid_to\": \"updates.valid_from\"\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd2_table_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "travel_booking_scd2_merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
